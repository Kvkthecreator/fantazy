"""
Deterministic Research Workflow Endpoint

Part of Workflow-First Architecture:
- Explicit parameters (no TP orchestration)
- Direct specialist invocation via ResearchExecutor
- Full context loading (on-demand substrate queries)
- Auditable execution tracking
- Optional recipe-driven execution (parameterized templates)

Architecture (Post-SDK Removal):
- Uses ResearchExecutor (direct Anthropic API, no Claude Agent SDK)
- First-principled context: work-oriented, no conversation persistence
- Tool execution via substrate-API HTTP calls
"""

from typing import Optional, Dict, Any
from uuid import UUID
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from datetime import datetime, timezone

from app.utils.jwt import verify_jwt
from app.utils.supabase_client import supabase_admin_client as supabase
from agents.research_executor import ResearchExecutor, create_research_executor
from services.recipe_loader import RecipeLoader, RecipeValidationError
import logging
import time

router = APIRouter(prefix="/work/research", tags=["workflows"])
logger = logging.getLogger(__name__)


class ResearchWorkflowRequest(BaseModel):
    """Deterministic research workflow parameters."""
    basket_id: str
    task_description: str
    research_scope: Optional[str] = "general"  # general, competitor, market, technical
    depth: Optional[str] = "standard"  # quick, standard, deep
    output_format: Optional[str] = "markdown"  # markdown, json, structured
    priority: Optional[int] = 5

    # Recipe integration (optional)
    recipe_id: Optional[str] = None  # Recipe UUID or slug
    recipe_parameters: Optional[Dict[str, Any]] = None  # User-customized parameters
    reference_asset_ids: Optional[list[str]] = None  # User-uploaded assets

    # Async execution mode
    async_execution: Optional[bool] = False  # If True, return ticket_id immediately


class ResearchWorkflowResponse(BaseModel):
    """Research workflow execution result."""
    work_request_id: str
    work_ticket_id: str
    status: str  # pending, running, completed, failed
    outputs: list[dict]  # work_outputs generated by agent
    execution_time_ms: Optional[int]
    message: str
    recipe_used: Optional[str] = None  # Recipe slug if recipe-driven
    token_usage: Optional[Dict[str, int]] = None  # Token usage stats


@router.post("/execute", response_model=ResearchWorkflowResponse)
async def execute_research_workflow(
    request: ResearchWorkflowRequest,
    user: dict = Depends(verify_jwt)
):
    """
    Execute deterministic research workflow.

    Flow:
    1. Validate permissions (workspace, basket)
    2. Create work_request + work_ticket (tracking)
    3. Execute ResearchExecutor with context
    4. Return structured outputs

    Args:
        request: Research workflow parameters
        user: Authenticated user from JWT

    Returns:
        Research workflow execution result with outputs

    Raises:
        401: Authentication failed
        403: Permission denied
        404: Basket or recipe not found
        400: Invalid recipe parameters
        500: Execution error
    """
    user_id = user.get("sub") or user.get("user_id")
    user_token = user.get("token")  # JWT for substrate-API auth
    if not user_id:
        raise HTTPException(status_code=401, detail="Invalid user token")

    logger.info(
        f"[RESEARCH WORKFLOW] Starting: user={user_id}, basket={request.basket_id}, "
        f"recipe={request.recipe_id}"
    )

    try:
        # Step 1: Validate basket access and get workspace
        basket_response = supabase.table("baskets").select(
            "id, workspace_id, name"
        ).eq("id", request.basket_id).single().execute()

        if not basket_response.data:
            raise HTTPException(status_code=404, detail="Basket not found")

        basket = basket_response.data
        workspace_id = basket["workspace_id"]

        # Step 2: Recipe-driven execution (if recipe_id provided)
        recipe = None
        execution_context = None
        validated_params = None

        if request.recipe_id:
            logger.info(f"[RESEARCH WORKFLOW] Loading recipe: {request.recipe_id}")

            loader = RecipeLoader()

            # Load recipe by ID or slug
            try:
                recipe = await loader.load_recipe(recipe_id=request.recipe_id)
            except Exception:
                recipe = await loader.load_recipe(slug=request.recipe_id)

            logger.info(f"[RESEARCH WORKFLOW] Loaded recipe: {recipe.name} (v{recipe.version})")

            # Validate parameters
            try:
                validated_params = loader.validate_parameters(
                    recipe=recipe,
                    user_parameters=request.recipe_parameters or {}
                )
                logger.info(f"[RESEARCH WORKFLOW] Validated parameters: {validated_params}")
            except RecipeValidationError as e:
                raise HTTPException(
                    status_code=400,
                    detail=f"Recipe parameter validation failed: {str(e)}"
                )

            # Generate execution context
            execution_context = loader.generate_execution_context(
                recipe=recipe,
                validated_parameters=validated_params
            )

        # Step 3: Create work_request (for tracking & billing)
        work_request_data = {
            "workspace_id": workspace_id,
            "basket_id": request.basket_id,
            "requested_by_user_id": user_id,
            "request_type": f"recipe_{recipe.slug}" if recipe else "research_workflow",
            "task_intent": recipe.name if recipe else request.task_description,
            "parameters": {
                "research_scope": request.research_scope,
                "depth": request.depth,
                "output_format": request.output_format,
                "recipe_used": recipe.slug if recipe else None,
                "recipe_parameters": validated_params if recipe else None,
            },
            "priority": "normal",
        }

        if recipe:
            work_request_data["recipe_id"] = recipe.id
            work_request_data["recipe_parameters"] = validated_params
            work_request_data["reference_asset_ids"] = request.reference_asset_ids or []

        work_request_response = supabase.table("work_requests").insert(
            work_request_data
        ).execute()
        work_request_id = work_request_response.data[0]["id"]

        # Step 4: Create work_ticket (execution tracking)
        work_ticket_data = {
            "work_request_id": work_request_id,
            "workspace_id": workspace_id,
            "basket_id": request.basket_id,
            "agent_type": "research",
            "status": "pending",
            "metadata": {
                "workflow": "recipe_research" if recipe else "deterministic_research",
                "task_description": request.task_description,
                "research_scope": request.research_scope,
                "depth": request.depth,
                "recipe_slug": recipe.slug if recipe else None,
                "recipe_id": recipe.id if recipe else None,
                "execution_mode": "direct_api",  # Mark as using new executor
            },
        }
        work_ticket_response = supabase.table("work_tickets").insert(
            work_ticket_data
        ).execute()
        work_ticket_id = work_ticket_response.data[0]["id"]

        logger.info(
            f"[RESEARCH WORKFLOW] Created: work_request={work_request_id}, "
            f"work_ticket={work_ticket_id}"
        )

        # ASYNC MODE: Return immediately, execute in background
        if request.async_execution:
            logger.info(f"[RESEARCH WORKFLOW] Async mode: returning ticket_id immediately")

            import threading
            import asyncio as bg_asyncio

            # Capture closure variables
            _basket_id = request.basket_id
            _workspace_id = workspace_id
            _user_id = user_id
            _user_token = user_token
            _work_request_id = work_request_id
            _work_ticket_id = work_ticket_id
            _task_description = request.task_description
            _research_scope = request.research_scope
            _depth = request.depth
            _recipe = recipe
            _execution_context = execution_context
            _validated_params = validated_params

            def execute_in_background():
                try:
                    from app.utils.supabase_client import supabase_admin_client as bg_supabase
                    from agents.research_executor import ResearchExecutor

                    # Update status to running
                    bg_supabase.table("work_tickets").update({
                        "status": "running",
                        "started_at": datetime.now(timezone.utc).isoformat(),
                    }).eq("id", _work_ticket_id).execute()

                    # Build enhanced task if recipe-driven
                    enhanced_task = _task_description
                    if _execution_context:
                        task_breakdown = _execution_context.get("task_breakdown", [])
                        deliverable_intent = _execution_context.get("deliverable_intent", {})
                        enhanced_task = f"""**Recipe: {_recipe.name}**

**Deliverable Intent:**
- Purpose: {deliverable_intent.get('purpose', 'Conduct research')}
- Audience: {deliverable_intent.get('audience', 'Decision-makers')}
- Outcome: {deliverable_intent.get('outcome', 'Structured findings')}

**Task:** {_task_description}

**Task Breakdown:**
{chr(10).join([f"- {step}" for step in task_breakdown])}

**Parameters:**
- Research Scope: {_validated_params.get('research_scope', 'general') if _validated_params else 'general'}
- Depth: {_validated_params.get('depth', 'standard') if _validated_params else 'standard'}
- Focus Area: {_validated_params.get('focus_area', 'None specified') if _validated_params else 'None specified'}

{_execution_context.get('system_prompt_additions', '')}
"""

                    # Create executor and run
                    executor = ResearchExecutor(
                        basket_id=_basket_id,
                        workspace_id=_workspace_id,
                        work_ticket_id=_work_ticket_id,
                        user_id=_user_id,
                        user_jwt=_user_token,
                    )

                    start_time = time.time()
                    result = bg_asyncio.run(executor.execute(
                        task=enhanced_task,
                        research_scope=_research_scope,
                        depth=_depth,
                    ))
                    execution_time_ms = int((time.time() - start_time) * 1000)

                    # Update to completed
                    existing_ticket = bg_supabase.table("work_tickets").select("metadata").eq("id", _work_ticket_id).single().execute()
                    existing_metadata = existing_ticket.data.get("metadata", {}) if existing_ticket.data else {}

                    updated_metadata = {
                        **existing_metadata,
                        "execution_time_ms": execution_time_ms,
                        "output_count": len(result.work_outputs),
                        "token_usage": {
                            "input_tokens": result.input_tokens,
                            "output_tokens": result.output_tokens,
                            "cache_read_tokens": result.cache_read_tokens,
                        },
                    }

                    bg_supabase.table("work_tickets").update({
                        "status": "completed",
                        "completed_at": datetime.now(timezone.utc).isoformat(),
                        "metadata": updated_metadata,
                    }).eq("id", _work_ticket_id).execute()

                    logger.info(f"[RESEARCH WORKFLOW] Background complete: {len(result.work_outputs)} outputs")

                except Exception as e:
                    logger.exception(f"[RESEARCH WORKFLOW] Background failed: {e}")
                    try:
                        from app.utils.supabase_client import supabase_admin_client as err_supabase
                        err_supabase.table("work_tickets").update({
                            "status": "failed",
                            "completed_at": datetime.now(timezone.utc).isoformat(),
                            "error_message": str(e),
                        }).eq("id", _work_ticket_id).execute()
                    except Exception as update_err:
                        logger.error(f"[RESEARCH WORKFLOW] Failed to update ticket: {update_err}")

            thread = threading.Thread(target=execute_in_background, daemon=True)
            thread.start()

            return ResearchWorkflowResponse(
                work_request_id=work_request_id,
                work_ticket_id=work_ticket_id,
                status="running",
                outputs=[],
                execution_time_ms=None,
                message="Research started in background - track progress via work_ticket status",
                recipe_used=recipe.slug if recipe else None,
            )

        # SYNC MODE: Execute and wait for result
        logger.info(f"[RESEARCH WORKFLOW] Sync mode: executing research")

        # Update status to running
        supabase.table("work_tickets").update({
            "status": "running",
            "started_at": datetime.now(timezone.utc).isoformat(),
        }).eq("id", work_ticket_id).execute()

        # Build enhanced task if recipe-driven
        enhanced_task = request.task_description
        if execution_context:
            task_breakdown = execution_context.get("task_breakdown", [])
            deliverable_intent = execution_context.get("deliverable_intent", {})
            enhanced_task = f"""**Recipe: {recipe.name}**

**Deliverable Intent:**
- Purpose: {deliverable_intent.get('purpose', 'Conduct research')}
- Audience: {deliverable_intent.get('audience', 'Decision-makers')}
- Outcome: {deliverable_intent.get('outcome', 'Structured findings')}

**Task:** {request.task_description}

**Task Breakdown:**
{chr(10).join([f"- {step}" for step in task_breakdown])}

**Parameters:**
- Research Scope: {validated_params.get('research_scope', 'general')}
- Depth: {validated_params.get('depth', 'standard')}
- Focus Area: {validated_params.get('focus_area', 'None specified')}

{execution_context.get('system_prompt_additions', '')}
"""

        # Create executor and run
        executor = create_research_executor(
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            work_ticket_id=work_ticket_id,
            user_id=user_id,
            user_jwt=user_token,
        )

        start_time = time.time()
        result = await executor.execute(
            task=enhanced_task,
            research_scope=request.research_scope,
            depth=request.depth,
        )
        execution_time_ms = int((time.time() - start_time) * 1000)

        # Update to completed
        supabase.table("work_tickets").update({
            "status": "completed",
            "completed_at": datetime.now(timezone.utc).isoformat(),
            "metadata": {
                "workflow": "recipe_research" if recipe else "deterministic_research",
                "execution_time_ms": execution_time_ms,
                "output_count": len(result.work_outputs),
                "recipe_slug": recipe.slug if recipe else None,
                "token_usage": {
                    "input_tokens": result.input_tokens,
                    "output_tokens": result.output_tokens,
                    "cache_read_tokens": result.cache_read_tokens,
                },
            },
        }).eq("id", work_ticket_id).execute()

        logger.info(
            f"[RESEARCH WORKFLOW] Complete: {len(result.work_outputs)} outputs "
            f"in {execution_time_ms}ms, tokens={result.input_tokens}+{result.output_tokens}"
        )

        return ResearchWorkflowResponse(
            work_request_id=work_request_id,
            work_ticket_id=work_ticket_id,
            status="completed",
            outputs=result.work_outputs,
            execution_time_ms=execution_time_ms,
            message=f"Research complete: {len(result.work_outputs)} outputs generated",
            recipe_used=recipe.slug if recipe else None,
            token_usage={
                "input_tokens": result.input_tokens,
                "output_tokens": result.output_tokens,
                "cache_read_tokens": result.cache_read_tokens,
            },
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"[RESEARCH WORKFLOW] Failed: {e}")

        if 'work_ticket_id' in locals():
            try:
                supabase.table("work_tickets").update({
                    "status": "failed",
                    "completed_at": datetime.now(timezone.utc).isoformat(),
                    "metadata": {
                        "error": str(e),
                        "error_type": type(e).__name__,
                        "recipe_slug": recipe.slug if recipe and 'recipe' in locals() else None,
                    },
                }).eq("id", work_ticket_id).execute()
            except Exception as update_error:
                logger.error(f"Failed to update work_ticket status: {update_error}")

        raise HTTPException(
            status_code=500,
            detail=f"Research workflow execution failed: {str(e)}"
        )
