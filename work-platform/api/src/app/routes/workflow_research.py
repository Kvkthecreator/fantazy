"""
Deterministic Research Workflow Endpoint

Part of Workflow-First Architecture (Phase 1):
- Explicit parameters (no TP orchestration)
- Direct specialist invocation
- Full context loading (WorkBundle pattern)
- Auditable execution tracking
- Optional recipe-driven execution (parameterized templates)

Recipe Integration:
- When recipe_id provided, validates parameters against recipe schema
- Generates execution context from recipe execution_template
- Passes context to ResearchAgentSDK.deep_dive()
- When no recipe, executes standard research flow
"""

from typing import Optional, Dict, Any
from uuid import UUID
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel

from app.utils.jwt import verify_jwt
from app.utils.supabase_client import supabase_admin_client as supabase
from agents_sdk.research_agent_sdk import ResearchAgentSDK
from agents_sdk.work_bundle import WorkBundle
from services.recipe_loader import RecipeLoader, RecipeValidationError
from shared.session import AgentSession
import logging

router = APIRouter(prefix="/work/research", tags=["workflows"])
logger = logging.getLogger(__name__)


class ResearchWorkflowRequest(BaseModel):
    """Deterministic research workflow parameters."""
    basket_id: str
    task_description: str
    research_scope: Optional[str] = "general"  # general, competitor, market, technical
    depth: Optional[str] = "standard"  # quick, standard, deep
    output_format: Optional[str] = "markdown"  # markdown, json, structured
    priority: Optional[int] = 5

    # Recipe integration (optional)
    recipe_id: Optional[str] = None  # Recipe UUID or slug
    recipe_parameters: Optional[Dict[str, Any]] = None  # User-customized parameters
    reference_asset_ids: Optional[list[str]] = None  # User-uploaded assets

    # Async execution mode
    async_execution: Optional[bool] = False  # If True, return ticket_id immediately


class ResearchWorkflowResponse(BaseModel):
    """Research workflow execution result."""
    work_request_id: str
    work_ticket_id: str
    agent_session_id: str
    status: str  # pending, running, completed, failed
    outputs: list[dict]  # work_outputs generated by agent
    execution_time_ms: Optional[int]
    message: str
    recipe_used: Optional[str] = None  # Recipe slug if recipe-driven


@router.post("/execute", response_model=ResearchWorkflowResponse)
async def execute_research_workflow(
    request: ResearchWorkflowRequest,
    user: dict = Depends(verify_jwt)
):
    """
    Execute deterministic research workflow.

    Flow (Standard):
    1. Validate permissions (workspace, basket, trial limits)
    2. Load context (WorkBundle: blocks + assets + config)
    3. Create work_request + work_ticket (tracking)
    4. Execute ResearchAgentSDK with context
    5. Return structured outputs

    Flow (Recipe-Driven):
    1. Load recipe by ID/slug
    2. Validate user parameters against recipe schema
    3. Generate execution context from recipe template
    4. Create work_request with recipe linkage
    5. Execute ResearchAgentSDK.deep_dive() with enhanced context
    6. Return structured outputs

    Args:
        request: Research workflow parameters
        user: Authenticated user from JWT

    Returns:
        Research workflow execution result with outputs

    Raises:
        401: Authentication failed
        403: Permission denied or trial limit exceeded
        404: Basket or recipe not found
        400: Invalid recipe parameters
        500: Execution error
    """
    user_id = user.get("sub") or user.get("user_id")
    user_token = user.get("token")  # JWT for substrate-API auth
    if not user_id:
        raise HTTPException(status_code=401, detail="Invalid user token")

    logger.info(
        f"[RESEARCH WORKFLOW] Starting: user={user_id}, basket={request.basket_id}, "
        f"recipe={request.recipe_id}"
    )

    try:
        # Step 1: Validate basket access and get workspace
        basket_response = supabase.table("baskets").select(
            "id, workspace_id, name"
        ).eq("id", request.basket_id).single().execute()

        if not basket_response.data:
            raise HTTPException(status_code=404, detail="Basket not found")

        basket = basket_response.data
        workspace_id = basket["workspace_id"]

        # TODO: Add workspace permission check (verify user has access)
        # TODO: Add trial limit check (work_requests count)

        # Step 2: Get or create research agent session (persistent per basket)
        research_session = await AgentSession.get_or_create(
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            agent_type="research",
            user_id=user_id,
        )

        logger.info(
            f"[RESEARCH WORKFLOW] Agent session: {research_session.id}"
        )

        # Step 3: Recipe-driven execution (if recipe_id provided)
        recipe = None
        execution_context = None
        validated_params = None

        if request.recipe_id:
            logger.info(f"[RESEARCH WORKFLOW] Loading recipe: {request.recipe_id}")

            loader = RecipeLoader()

            # Load recipe by ID or slug
            try:
                # Try as UUID first
                recipe = await loader.load_recipe(recipe_id=request.recipe_id)
            except Exception:
                # Fall back to slug (handles both RecipeValidationError and UUID parsing errors)
                recipe = await loader.load_recipe(slug=request.recipe_id)

            logger.info(f"[RESEARCH WORKFLOW] Loaded recipe: {recipe.name} (v{recipe.version})")

            # Validate parameters
            try:
                validated_params = loader.validate_parameters(
                    recipe=recipe,
                    user_parameters=request.recipe_parameters or {}
                )
                logger.info(f"[RESEARCH WORKFLOW] Validated parameters: {validated_params}")
            except RecipeValidationError as e:
                raise HTTPException(
                    status_code=400,
                    detail=f"Recipe parameter validation failed: {str(e)}"
                )

            # Generate execution context
            execution_context = loader.generate_execution_context(
                recipe=recipe,
                validated_parameters=validated_params
            )

        # Step 4: Create work_request (for trial tracking & billing)
        work_request_data = {
            "workspace_id": workspace_id,
            "basket_id": request.basket_id,
            "requested_by_user_id": user_id,
            "request_type": f"recipe_{recipe.slug}" if recipe else "research_workflow",
            "task_intent": recipe.name if recipe else request.task_description,
            "parameters": {
                "research_scope": request.research_scope,
                "depth": request.depth,
                "output_format": request.output_format,
                "recipe_used": recipe.slug if recipe else None,
                "recipe_parameters": validated_params if recipe else None,
            },
            "priority": "normal",
        }

        # Add recipe linkage if recipe-driven
        if recipe:
            work_request_data["recipe_id"] = recipe.id
            work_request_data["recipe_parameters"] = validated_params
            work_request_data["reference_asset_ids"] = request.reference_asset_ids or []

        work_request_response = supabase.table("work_requests").insert(
            work_request_data
        ).execute()
        work_request_id = work_request_response.data[0]["id"]

        # Step 5: Create work_ticket (execution tracking)
        work_ticket_data = {
            "work_request_id": work_request_id,
            "agent_session_id": research_session.id,
            "workspace_id": workspace_id,
            "basket_id": request.basket_id,
            "agent_type": "research",
            "status": "pending",
            "metadata": {
                "workflow": "recipe_research" if recipe else "deterministic_research",
                "task_description": request.task_description,
                "research_scope": request.research_scope,
                "depth": request.depth,
                "recipe_slug": recipe.slug if recipe else None,
                "recipe_id": recipe.id if recipe else None,
            },
        }
        work_ticket_response = supabase.table("work_tickets").insert(
            work_ticket_data
        ).execute()
        work_ticket_id = work_ticket_response.data[0]["id"]

        logger.info(
            f"[RESEARCH WORKFLOW] Created: work_request={work_request_id}, "
            f"work_ticket={work_ticket_id}"
        )

        # Step 6: Create WorkBundle (metadata only) + SubstrateQueryAdapter (on-demand)
        logger.info(f"[RESEARCH WORKFLOW] Creating context for basket {request.basket_id}")

        # 6a. Load prior work outputs (recent research to avoid duplication)
        prior_outputs_response = supabase.table("work_outputs").select(
            "id, title, output_type, body, confidence, created_at"
        ).eq("basket_id", request.basket_id).eq(
            "agent_type", "research"
        ).eq("supervision_status", "approved").order(
            "created_at", desc=True
        ).limit(50).execute()

        prior_work_outputs = prior_outputs_response.data or []
        logger.info(f"[RESEARCH WORKFLOW] Loaded {len(prior_work_outputs)} prior outputs")

        # 6b. Load reference assets (documents, screenshots, etc.)
        assets_response = supabase.table("documents").select(
            "id, title, document_type, metadata"
        ).eq("basket_id", request.basket_id).execute()

        reference_assets = assets_response.data or []
        logger.info(f"[RESEARCH WORKFLOW] Loaded {len(reference_assets)} reference assets")

        # 6c. Create WorkBundle (metadata only - NO substrate_blocks)
        # Agents query substrate on-demand via SubstrateQueryAdapter
        context_bundle = WorkBundle(
            work_request_id=work_request_id,
            work_ticket_id=work_ticket_id,
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            user_id=user_id,
            task=request.task_description,
            agent_type="research",
            priority="medium",
            reference_assets=reference_assets,
            agent_config={},  # Use defaults
        )

        # 6d. Create SubstrateQueryAdapter for on-demand substrate access
        from adapters.substrate_adapter import SubstrateQueryAdapter
        substrate_adapter = SubstrateQueryAdapter(
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            user_token=user_token,  # Pass JWT for substrate-API auth
            agent_type="research",
            work_ticket_id=work_ticket_id,
        )

        logger.info(
            f"[RESEARCH WORKFLOW] Context created: "
            f"{len(reference_assets)} assets, {len(prior_work_outputs)} prior outputs, "
            f"SubstrateQueryAdapter for on-demand queries"
        )

        # Step 7: Update work_ticket status to running
        supabase.table("work_tickets").update({
            "status": "running",
            "started_at": "now()",
        }).eq("id", work_ticket_id).execute()

        # Step 8: Execute ResearchAgentSDK with context
        logger.info(f"[RESEARCH WORKFLOW] Executing ResearchAgentSDK")

        # Initialize ResearchAgentSDK with bundle + substrate adapter
        research_sdk = ResearchAgentSDK(
            basket_id=request.basket_id,
            workspace_id=workspace_id,
            work_ticket_id=work_ticket_id,
            session=research_session,
            substrate=substrate_adapter,  # On-demand substrate queries
            bundle=context_bundle,  # WorkBundle (metadata only)
        )

        # Build enhanced prompt with recipe context or prior work
        enhanced_task = request.task_description

        # If recipe-driven, add execution context
        if execution_context:
            task_breakdown = execution_context.get("task_breakdown", [])
            deliverable_intent = execution_context.get("deliverable_intent", {})

            enhanced_task = f"""**Recipe: {recipe.name}**

**Deliverable Intent:**
- Purpose: {deliverable_intent.get('purpose', 'Conduct research')}
- Audience: {deliverable_intent.get('audience', 'Decision-makers')}
- Outcome: {deliverable_intent.get('outcome', 'Structured findings')}

**Task:** {request.task_description}

**Task Breakdown:**
{chr(10).join([f"- {step}" for step in task_breakdown])}

**Parameters:**
- Research Scope: {validated_params.get('research_scope', 'general')}
- Depth: {validated_params.get('depth', 'standard')}
- Focus Area: {validated_params.get('focus_area', 'None specified')}

{execution_context.get('system_prompt_additions', '')}
"""
        elif prior_work_outputs:
            enhanced_task += "\n\n**Prior Research** (avoid duplication):\n"
            for output in prior_work_outputs[:5]:  # Show last 5
                enhanced_task += f"- {output['title']} ({output['output_type']})\n"

        # Execute deep dive research
        import time
        start_time = time.time()

        result = await research_sdk.deep_dive(
            topic=enhanced_task,
            claude_session_id=research_session.claude_session_id,
        )

        execution_time_ms = int((time.time() - start_time) * 1000)

        # Get final TodoWrite state from result
        final_todos = result.get("final_todos", [])

        # Step 9: Update work_ticket status to completed
        supabase.table("work_tickets").update({
            "status": "completed",
            "completed_at": "now()",
            "metadata": {
                "workflow": "recipe_research" if recipe else "deterministic_research",
                "execution_time_ms": execution_time_ms,
                "output_count": result["output_count"],
                "claude_session_id": result.get("claude_session_id"),
                "final_todos": final_todos,  # Store historical TodoWrite data
                "recipe_slug": recipe.slug if recipe else None,
            },
        }).eq("id", work_ticket_id).execute()

        logger.info(
            f"[RESEARCH WORKFLOW] Execution complete: {result['output_count']} outputs "
            f"in {execution_time_ms}ms"
        )

        return ResearchWorkflowResponse(
            work_request_id=work_request_id,
            work_ticket_id=work_ticket_id,
            agent_session_id=research_session.id,
            status="completed",
            outputs=result["work_outputs"],
            execution_time_ms=execution_time_ms,
            message=f"Research complete: {result['output_count']} outputs generated",
            recipe_used=recipe.slug if recipe else None,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.exception(f"[RESEARCH WORKFLOW] Failed: {e}")

        # Update work_ticket to failed status if it exists
        if 'work_ticket_id' in locals():
            try:
                supabase.table("work_tickets").update({
                    "status": "failed",
                    "completed_at": "now()",
                    "metadata": {
                        "error": str(e),
                        "error_type": type(e).__name__,
                        "recipe_slug": recipe.slug if recipe and 'recipe' in locals() else None,
                    },
                }).eq("id", work_ticket_id).execute()
            except Exception as update_error:
                logger.error(f"Failed to update work_ticket status: {update_error}")

        raise HTTPException(
            status_code=500,
            detail=f"Research workflow execution failed: {str(e)}"
        )
